{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "from camel_tools.tagger.default import DefaultTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tweets(path):\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    t = []\n",
    "    l = []\n",
    "    for token in open(path, encoding='utf-8').read().splitlines(): \n",
    "        if token == '':\n",
    "            tokens.append(t)\n",
    "            labels.append(l)\n",
    "            t = []\n",
    "            l = []\n",
    "            continue\n",
    "        splits = token.split()\n",
    "        t.append(splits[0])\n",
    "        l.append(splits[1])\n",
    "\n",
    "    if len(t) > 0 and len(l) > 0:\n",
    "        tokens.append(t)\n",
    "        labels.append(l)\n",
    "        \n",
    "    return tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tags: https://camel-tools.readthedocs.io/en/latest/reference/camel_morphology_features.html\n",
    "tag_mapping = {\"adj\": \"ADJ\", \"adposition\": \"ADP\", \"adverb\": \"ADV\", \"auxiliary\": \"AUX\", \n",
    "               \"coordinating conjunction\": \"CCONJ\", \"determiner\": \"DET\", \"interjection\": \"INTJ\", \n",
    "               \"noun\": \"NOUN\", \"numeral\": \"NUM\", \"particle\": \"PART\", \"pronoun\": \"PRON\", \n",
    "               \"proper noun\": \"PROPN\", \"punc\": \"PUNCT\", \"subordinating conjunction\": \"SCONJ\", \n",
    "               \"symbol\": \"SYM\", \"verb\": \"VERB\", \"other\": \"X\"}\n",
    "\n",
    "mled = MLEDisambiguator.pretrained()\n",
    "tagger = DefaultTagger(mled, 'pos')\n",
    "print(tagger.feature_list())\n",
    "\n",
    "def get_ar_pos(tokens):\n",
    "    pos = []\n",
    "    for i in range(0, len(tokens)):\n",
    "        t_pos = tagger.tag(tokens[i])\n",
    "        if len(t_pos) != len(tokens[i]):\n",
    "            print(\"mismatch in length\")\n",
    "        pos.append(t_pos)\n",
    "    return pos\n",
    "\n",
    "def get_en_pos(tokens):\n",
    "    pos = []\n",
    "    for i in range(0, len(tokens)):\n",
    "        t_pos = []\n",
    "        for e in nltk.pos_tag(tokens[i]):\n",
    "            for p in e[1].split(\"\\n\"):\n",
    "                t_pos.append(p)\n",
    "        if len(t_pos) != len(tokens[i]):\n",
    "            print(\"mismatch in length\")\n",
    "        pos.append(t_pos)\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conver2crf(tokens, pos, labels, out_path):\n",
    "    with open(out_path, mode='w', encoding=\"utf-8\", newline=\"\") as data_file:\n",
    "        writer = csv.writer(data_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow([\"Sentence #\",\"Word\",\"POS\",\"Tag\"])\n",
    "        for tweet in range(0, len(tokens)):\n",
    "            begin = True\n",
    "            for j in range(0, len(tokens[tweet])):\n",
    "                if begin:\n",
    "                    writer.writerow([\"Sentence: \" + str(tweet+1), tokens[tweet][j], pos[tweet][j], labels[tweet][j]])\n",
    "                    begin = False\n",
    "                else:\n",
    "                    writer.writerow([\"\", tokens[tweet][j], pos[tweet][j], labels[tweet][j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2typeless(labels):\n",
    "    tllabels = []\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        tllabels.append([])\n",
    "        for l in labels[i]:\n",
    "            spls = l.split(\"-\")\n",
    "            if len(spls) > 1:\n",
    "                tllabels[i].append(spls[0]+\"-LOC\")\n",
    "            else:\n",
    "                tllabels[i].append(l)\n",
    "    \n",
    "    return tllabels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"<path to IDRISI data directory>\" + \"IDRISI\\\\data\\\\LMR\\\\\"\n",
    "events = [\"beirut_explosion_2020\", \"cairo_bombing_2019\", \"covid_2019\", \"dragon_storms_2020\",\n",
    "          \"hafr_albatin_floods_2019\", \"jordan_floods_2018\", \"kuwait_floods_2018\"]\n",
    "\n",
    "for typ in ['typefull', 'typeless']:\n",
    "    for case in ['random', 'timebased']:\n",
    "        for event in events:\n",
    "            in_path = path + \"AR\\gold-\" + case + \"-bilou\\\\\" + event \n",
    "            out_path = path + \"AR\\gold-\" + case + \"-bilou-crf\\\\\" + typ + \"\\\\\" + event \n",
    "            if not os.path.exists(out_path):\n",
    "                os.makedirs(out_path)\n",
    "            for prt in [\"train\", \"dev\", \"test\"]:\n",
    "                prt_in_path = in_path + \"\\\\\" + prt + \".txt\"\n",
    "                tokens, labels = read_tweets(prt_in_path)\n",
    "                pos = get_ar_pos(tokens)\n",
    "                if typ == 'typeless':\n",
    "                    labels = convert2typeless(labels)\n",
    "                prt_out_path = out_path + \"\\\\\" + prt + \".csv\"\n",
    "                conver2crf(tokens, pos, labels, prt_out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"<path to IDRISI data directory>\" + \"IDRISI\\\\data\\\\LMR\\\\\"\n",
    "events = [\"california_wildfires_2018\", \"canada_wildfires_2016\", \"cyclone_idai_2019\", \"ecuador_earthquake_2016\", \n",
    "          \"greece_wildfires_2018\", \"hurricane_dorian_2019\", \"hurricane_florence_2018\", \"hurricane_harvey_2017\", \n",
    "          \"hurricane_irma_2017\", \"hurricane_maria_2017\", \"hurricane_matthew_2016\", \"italy_earthquake_aug_2016\", \n",
    "          \"kaikoura_earthquake_2016\", \"kerala_floods_2018\", \"maryland_floods_2018\", \"midwestern_us_floods_2019\", \n",
    "          \"pakistan_earthquake_2019\", \"puebla_mexico_earthquake_2017\", \"srilanka_floods_2017\"]\n",
    "\n",
    "for typ in ['typefull', 'typeless']:\n",
    "    for case in ['random', 'timebased']:\n",
    "        for event in events:\n",
    "\n",
    "            in_path = path + \"EN\\gold-\" + case + \"-bilou\\\\\" + event \n",
    "            out_path = path + \"EN\\gold-\" + case + \"-bilou-crf\\\\\" + typ + \"\\\\\" + event \n",
    "            if not os.path.exists(out_path):\n",
    "                os.makedirs(out_path)\n",
    "            for prt in [\"train\", \"dev\", \"test\"]:\n",
    "                prt_in_path = in_path + \"\\\\\" + prt + \".txt\"\n",
    "                tokens, labels = read_tweets(prt_in_path)\n",
    "                pos = get_en_pos(tokens)\n",
    "                if typ == 'typeless':\n",
    "                    labels = convert2typeless(labels)\n",
    "                prt_out_path = out_path + \"\\\\\" + prt + \".csv\"\n",
    "                conver2crf(tokens, pos, labels, prt_out_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
