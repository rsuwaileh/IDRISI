{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "types = {\"Island\": \"ISL\", \"State\": \"STAT\", \"Continent\": \"CONT\", \"City/town\": \"CITY\", \"Country\": \"CTRY\",\n",
    "        \"County\": \"CNTY\", \"Neighborhood\": \"NBHD\", \"Road/street\": \"ST\", \"District\": \"DIST\", \"Other locations\": \"OTHR\", \n",
    "        \"Natural Point-of-Interest\": \"NPOI\", \"Human-made Point-of-Interest\": \"HPOI\"}\n",
    "\n",
    "en_events = [\"california_wildfires_2018\", \"canada_wildfires_2016\", \"cyclone_idai_2019\", \"ecuador_earthquake_2016\",\n",
    "             \"greece_wildfires_2018\", \"hurricane_dorian_2019\", \"hurricane_florence_2018\", \"hurricane_harvey_2017\",\n",
    "             \"hurricane_irma_2017\", \"hurricane_maria_2017\", \"hurricane_matthew_2016\", \"italy_earthquake_aug_2016\",\n",
    "             \"kaikoura_earthquake_2016\", \"kerala_floods_2018\", \"maryland_floods_2018\", \"midwestern_us_floods_2019\",\n",
    "             \"pakistan_earthquake_2019\", \"puebla_mexico_earthquake_2017\", \"srilanka_floods_2017\"]\n",
    "\n",
    "ar_events = [\"beirut_explosion_2020\", \"cairo_bombing_2019\", \"covid_2019\", \"dragon_storms_2020\", \n",
    "             \"hafr_albatin_floods_2019\", \"jordan_floods_2018\", \"kuwait_floods_2018\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads the BILOU formatted data\n",
    "\n",
    "def read_bilou(path):\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    t = []\n",
    "    l = []\n",
    "    \n",
    "    for token in open(path, encoding='utf-8').read().splitlines(): \n",
    "        if token == '':\n",
    "            tokens.append(t)\n",
    "            labels.append(l)\n",
    "            t = []\n",
    "            l = []\n",
    "            continue\n",
    "        splits = token.split()\n",
    "        t.append(splits[0])\n",
    "        l.append(splits[1])\n",
    "        \n",
    "    if len(t) > 0 and len(l) > 0:\n",
    "        tokens.append(t)\n",
    "        labels.append(l) \n",
    "        \n",
    "    return tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    punct = re.compile(r'(\\w+)')\n",
    "    clean_text = ' '.join([m.group() for m in punct.finditer(text)])\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json(path):\n",
    "    locs = []\n",
    "    lines = open(path, encoding='utf-8').read().splitlines()\n",
    "    for line in lines:\n",
    "        tweet = json.loads(line)\n",
    "        locs.append([remove_punct(lm[\"text\"]).lower().replace(\" \",\"\") for lm in tweet[\"location_mentions\"]])\n",
    "    return locs\n",
    "\n",
    "def parse_json_by_type(path, LOC):\n",
    "    locs = []\n",
    "    lines = open(path, encoding='utf-8').read().splitlines()\n",
    "    for line in lines:\n",
    "        tweet = json.loads(line)\n",
    "        if LOC:\n",
    "            locs.append([(remove_punct(lm[\"text\"]).lower().replace(\" \",\"\"), \"LOC\") for lm in tweet[\"location_mentions\"]])\n",
    "        else:\n",
    "            locs.append([(remove_punct(lm[\"text\"]).lower().replace(\" \",\"\"), types[lm[\"type\"]]) for lm in tweet[\"location_mentions\"]])\n",
    "    return locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bilou(path):\n",
    "    tokens, labels = read_bilou(path)\n",
    "        \n",
    "    locs = []\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        loc = \"\"\n",
    "        local_locs = []\n",
    "        for j in range(len(tokens[i])):\n",
    "            if \"U-\" in labels[i][j]:\n",
    "                loc = tokens[i][j]#.replace(\" ’\", \"’\").replace(\" - \", \"-\").replace(\"# \", \"#\").replace(\" ,\", \",\").replace(\" / \", \"/\")\n",
    "                local_locs.append(remove_punct(loc).lower().replace(\" \",\"\"))\n",
    "                loc = \"\"\n",
    "            elif \"B-\" in labels[i][j]: #malformed BIO-LM will be ignored\n",
    "                loc = tokens[i][j]\n",
    "            elif \"I-\" in labels[i][j]:\n",
    "                if loc == \"\": #malformed BIO-LM will be ignored\n",
    "                    continue \n",
    "                loc += \" \" + tokens[i][j]\n",
    "            elif \"L-\" in labels[i][j]:\n",
    "                if loc == \"\": #malformed BIO-LM will be ignored\n",
    "                    continue \n",
    "                loc += \" \" + tokens[i][j]\n",
    "                #local_locs.append(loc.replace(\" ’\", \"’\").replace(\" - \", \"-\").replace(\"# \", \"#\").replace(\" ,\", \",\").replace(\" / \", \"/\"))\n",
    "                local_locs.append(remove_punct(loc).lower().replace(\" \",\"\"))\n",
    "                loc = \"\"\n",
    "            else:\n",
    "                loc = \"\"  #malformed BIO-LM will be ignored\n",
    "        locs.append(local_locs)\n",
    "    return locs\n",
    "\n",
    "def parse_bilou_by_type(path):\n",
    "    tokens, labels = read_bilou(path)\n",
    "    \n",
    "    locs = []\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        loc = \"\"\n",
    "        local_locs = []\n",
    "        for j in range(len(tokens[i])):\n",
    "            if \"U-\" in labels[i][j]:\n",
    "                loc = tokens[i][j]\n",
    "                local_locs.append((remove_punct(loc).lower().replace(\" \",\"\"), labels[i][j].split(\"-\")[1]))\n",
    "                loc = \"\"\n",
    "            elif \"B-\" in labels[i][j]: #malformed BIO-LM will be ignored\n",
    "                loc = tokens[i][j]\n",
    "            elif \"I-\" in labels[i][j]:\n",
    "                if loc == \"\": #malformed BIO-LM will be ignored\n",
    "                    continue \n",
    "                loc += \" \" + tokens[i][j]\n",
    "            elif \"L-\" in labels[i][j]:\n",
    "                if loc == \"\": #malformed BIO-LM will be ignored\n",
    "                    continue\n",
    "                loc += \" \" + tokens[i][j]\n",
    "                local_locs.append((remove_punct(loc).lower().replace(\" \",\"\"), labels[i][j].split(\"-\")[1]))\n",
    "                loc = \"\"\n",
    "            else:\n",
    "                loc = \"\"  #malformed BIO-LM will be ignored\n",
    "                \n",
    "        locs.append(local_locs)\n",
    "    return locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bio(path):\n",
    "    tokens, labels = read_bilou(path)\n",
    "        \n",
    "    locs = []\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        loc = \"\"\n",
    "        local_locs = []\n",
    "        for j in range(len(tokens[i])):\n",
    "            if labels[i][j] != \"O\":\n",
    "                try:\n",
    "                    if labels[i][j].split(\"-\")[1] in ['PERS', 'ORG']:\n",
    "                        continue\n",
    "                except:\n",
    "                    print(labels[i][j])\n",
    "            \n",
    "            if \"B-\" in labels[i][j]: #malformed BIO-LM will be ignored\n",
    "                if loc != \"\":\n",
    "                    local_locs.append(remove_punct(loc).lower().replace(\" \",\"\"))\n",
    "                loc = tokens[i][j]\n",
    "            elif \"I-\" in labels[i][j]:\n",
    "                if loc == \"\": #malformed BIO-LM will be ignored\n",
    "                    continue \n",
    "                loc += \" \" + tokens[i][j]\n",
    "            else:\n",
    "                if loc != \"\":\n",
    "                    local_locs.append(remove_punct(loc).lower().replace(\" \",\"\"))\n",
    "                loc = \"\"  #malformed BIO-LM will be ignored\n",
    "        locs.append(local_locs)\n",
    "    return locs\n",
    "\n",
    "def parse_bio_by_type(path):\n",
    "    tokens, labels = read_bilou(path)\n",
    "    \n",
    "    locs = []\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        loc = \"\"\n",
    "        local_locs = []\n",
    "        for j in range(len(tokens[i])):\n",
    "            if \"B-\" in labels[i][j]: #malformed BIO-LM will be ignored\n",
    "                if loc != \"\":\n",
    "                    local_locs.append((remove_punct(loc).lower().replace(\" \",\"\"), labels[i][j].split(\"-\")[1]))\n",
    "                loc = tokens[i][j]\n",
    "            elif \"I-\" in labels[i][j]:\n",
    "                if loc == \"\": #malformed BIO-LM will be ignored\n",
    "                    continue \n",
    "                loc += \" \" + tokens[i][j]\n",
    "            else:\n",
    "                if loc != \"\":\n",
    "                    local_locs.append((remove_punct(loc).lower().replace(\" \",\"\"), labels[i][j].split(\"-\")[1]))\n",
    "                loc = \"\"  #malformed BIO-LM will be ignored\n",
    "        locs.append(local_locs)\n",
    "    return locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bilou(path):\n",
    "    tokens, labels = read_bilou(path)\n",
    "        \n",
    "    locs = []\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        loc = \"\"\n",
    "        local_locs = []\n",
    "        for j in range(len(tokens[i])):\n",
    "            if \"U-\" in labels[i][j]:\n",
    "                loc = tokens[i][j]#.replace(\" ’\", \"’\").replace(\" - \", \"-\").replace(\"# \", \"#\").replace(\" ,\", \",\").replace(\" / \", \"/\")\n",
    "                local_locs.append(remove_punct(loc).lower().replace(\" \",\"\"))\n",
    "                loc = \"\"\n",
    "            elif \"B-\" in labels[i][j]: #malformed BIO-LM will be ignored\n",
    "                loc = tokens[i][j]\n",
    "            elif \"I-\" in labels[i][j]:\n",
    "                if loc == \"\": #malformed BIO-LM will be ignored\n",
    "                    continue \n",
    "                loc += \" \" + tokens[i][j]\n",
    "            elif \"L-\" in labels[i][j]:\n",
    "                if loc == \"\": #malformed BIO-LM will be ignored\n",
    "                    continue \n",
    "                loc += \" \" + tokens[i][j]\n",
    "                #local_locs.append(loc.replace(\" ’\", \"’\").replace(\" - \", \"-\").replace(\"# \", \"#\").replace(\" ,\", \",\").replace(\" / \", \"/\"))\n",
    "                local_locs.append(remove_punct(loc).lower().replace(\" \",\"\"))\n",
    "                loc = \"\"\n",
    "            else:\n",
    "                loc = \"\"  #malformed BIO-LM will be ignored\n",
    "        locs.append(local_locs)\n",
    "    return locs\n",
    "\n",
    "def parse_bilou_by_type(path):\n",
    "    tokens, labels = read_bilou(path)\n",
    "    \n",
    "    locs = []\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        loc = \"\"\n",
    "        local_locs = []\n",
    "        for j in range(len(tokens[i])):\n",
    "            if \"U-\" in labels[i][j]:\n",
    "                loc = tokens[i][j]\n",
    "                local_locs.append((remove_punct(loc).lower().replace(\" \",\"\"), labels[i][j].split(\"-\")[1]))\n",
    "                loc = \"\"\n",
    "            elif \"B-\" in labels[i][j]: #malformed BIO-LM will be ignored\n",
    "                loc = tokens[i][j]\n",
    "            elif \"I-\" in labels[i][j]:\n",
    "                if loc == \"\": #malformed BIO-LM will be ignored\n",
    "                    continue \n",
    "                loc += \" \" + tokens[i][j]\n",
    "            elif \"L-\" in labels[i][j]:\n",
    "                if loc == \"\": #malformed BIO-LM will be ignored\n",
    "                    continue\n",
    "                loc += \" \" + tokens[i][j]\n",
    "                local_locs.append((remove_punct(loc).lower().replace(\" \",\"\"), labels[i][j].split(\"-\")[1]))\n",
    "                loc = \"\"\n",
    "            else:\n",
    "                loc = \"\"  #malformed BIO-LM will be ignored\n",
    "                \n",
    "        locs.append(local_locs)\n",
    "    return locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_unpredicted_seq(gl, pl):\n",
    "    if len(gl) != len(pl):\n",
    "        raise NameError(\"Found input variables with inconsistent numbers of samples! len(gold) = %d and len(pred) = %d\" % (len(gl), len(pl)))\n",
    "\n",
    "    for i in range(len(gl)):\n",
    "        if len(pl[i]) < len(gl[i]):\n",
    "            rem = len(gl[i]) - len(pl[i])\n",
    "            for r in range(rem):\n",
    "                pl[i].append(\"O\")\n",
    "    return pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_matrix_per_example(gold, pred):\n",
    "    TPs = []\n",
    "    FPs = []\n",
    "    FNs = []\n",
    "    \n",
    "    for i in range(len(gold)):\n",
    "        ''' FIXME find more efficient and correct'''\n",
    "        \n",
    "        # in some cases the prediction contains same LM twice while it appear once in gold.\n",
    "        TP = []\n",
    "        FP = []\n",
    "        #TN = []\n",
    "        FN = []\n",
    "        for l in pred[i]:\n",
    "            if l in gold[i]:\n",
    "                if gold[i].count(l) == TP.count(l):\n",
    "                    FP.append(l)\n",
    "                    continue\n",
    "                else: #l not in TP yet\n",
    "                    TP.append(l)\n",
    "            else:\n",
    "                FP.append(l)\n",
    "                \n",
    "        TPs.append(TP)\n",
    "        FPs.append(FP)\n",
    "        \n",
    "        for l in gold[i]:\n",
    "            if l not in pred[i]:\n",
    "                if pred[i].count(l) == FN.count(l):\n",
    "                    #TN.append(l)\n",
    "                    continue\n",
    "                else: #l not in FN yet\n",
    "                    FN.append(l)\n",
    "            #else:\n",
    "            #    TN.append(l)\n",
    "                \n",
    "        FNs.append(FN)\n",
    "        \n",
    "        \n",
    "    #print(TPs)\n",
    "    #print(FPs)\n",
    "    #print(FNs)\n",
    "    \n",
    "        \n",
    "    return TPs, FPs, FNs\n",
    "\n",
    "def count_matrix_per_example(gold, pred):\n",
    "    \n",
    "    TPs, FPs, FNs = extract_matrix_per_example(gold, pred)\n",
    "    TPs_counts = [len(tp) for tp in TPs]\n",
    "    FPs_counts = [len(fp) for fp in FPs]\n",
    "    FNs_counts = [len(fn) for fn in FNs]\n",
    "\n",
    "    return TPs_counts, FPs_counts, FNs_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision(gold, pred, mode, internal=False):\n",
    "    \n",
    "    if len(gold) != len(pred):\n",
    "        raise NameError(\"Found input variables with inconsistent numbers of samples! len(gold) = %d and len(pred) = %d\" \n",
    "                        % (len(gold), len(pred)))\n",
    "    \n",
    "    # FIXME count this ONCE for all eval measures!\n",
    "    TPs, FPs, FNs = count_matrix_per_example(gold, pred)\n",
    "    ps = []\n",
    "    if mode == 'special':\n",
    "        for i in range(len(TPs)):\n",
    "            #special case where no LMs in the gold and the system outputs nothing\n",
    "            #it's commented because we're evaluting for the positive label (LOC) here\n",
    "            if list(set(gold[i] + pred[i])) == []:\n",
    "                #print(\"special case\")\n",
    "                p = 1.00\n",
    "            else:\n",
    "                den = len(pred[i]) # FIXME can be replaced with TPs[i] + FPs[i]\n",
    "                p = TPs[i]/den if den > 0 else 0\n",
    "            \n",
    "            ps.append(p)\n",
    "    else:    \n",
    "        for i in range(len(TPs)):\n",
    "            den = len(pred[i])  # FIXME can be replaced with TPs[i] + FPs[i]\n",
    "            p = TPs[i]/den if den > 0 else 0 \n",
    "            ps.append(p)\n",
    "    #print(ps)\n",
    "    if internal:\n",
    "        return ps\n",
    "    else:\n",
    "        return sum(ps)/len(ps)\n",
    "\n",
    "def compute_recall(gold, pred, mode, internal=False):\n",
    "    if len(gold) != len(pred):\n",
    "        raise NameError(\"Found input variables with inconsistent numbers of samples! len(gold) = %d and len(pred) = %d\" \n",
    "                        % (len(gold), len(pred)))\n",
    "    \n",
    "    TPs, FPs, FNs = count_matrix_per_example(gold, pred)\n",
    "    #print([TPs, FPs, FNs])\n",
    "    \n",
    "    rs = []\n",
    "    if mode == 'special':\n",
    "        #special case where no LMs in the gold and the system outputs nothing\n",
    "        #it's commented because we're evaluting for the positive label (LOC) here\n",
    "        for i in range(len(TPs)):\n",
    "            if list(set(gold[i] + pred[i])) == []:\n",
    "                #print(\"special case\")\n",
    "                r = 1.00\n",
    "            else:        \n",
    "                den = len(gold[i])  # FIXME can be replaced with TPs[i] + FNs[i]\n",
    "                r = TPs[i]/den if den > 0 else 0\n",
    "            rs.append(r)\n",
    "    else:\n",
    "        for i in range(len(TPs)):      \n",
    "            den =  len(gold[i]) # FIXME can be replaced with  TPs[i] + FNs[i]\n",
    "            r = TPs[i]/den if den > 0 else 0\n",
    "            rs.append(r)\n",
    "    #print(rs)\n",
    "    if internal:\n",
    "        return rs\n",
    "    else:\n",
    "        return sum(rs)/len(rs)\n",
    "\n",
    "def compute_fscore(beta, gold, pred, mode):\n",
    "    ps = compute_precision(gold, pred, mode, True)\n",
    "    rs = compute_recall(gold, pred, mode, True)\n",
    "\n",
    "    fs = []\n",
    "    for i in range(len(ps)):\n",
    "        if ps[i] + rs[i] == 0.00:\n",
    "            f = 0.00\n",
    "        else:\n",
    "            beta2 = beta**2\n",
    "            f = ((1 + beta2) * ps[i] * rs[i]) / (beta2 * ps[i] + rs[i])\n",
    "        fs.append(f)\n",
    "        #print(\"f = %f\" % f)\n",
    "    return sum(fs)/len(fs)\n",
    "\n",
    "\n",
    "def count_special(gold, pred):\n",
    "    count = 0\n",
    "    for i in range(len(gold)):\n",
    "        if list(set(gold[i] + pred[i])) == []:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typeless evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(gold_path, gold_file_format, pred_path, pred_file_format, beta, mode, per_dataset):\n",
    "    #DRIVER code\n",
    "\n",
    "    # TODO 1. read the gold data \n",
    "    #         --> parameters: path, data format (BILOU, JSON)\n",
    "    #         --> output gold: [[locs]], pred: [[locs]]\n",
    "    if gold_file_format == \"bilou\":\n",
    "        gold = parse_bilou(gold_path)\n",
    "    elif gold_file_format == \"bio\":\n",
    "        gold = parse_bio(gold_path)\n",
    "    else:\n",
    "        gold = parse_json(gold_path)\n",
    "    \n",
    "    if pred_file_format == \"bilou\":\n",
    "        pred = parse_bilou(pred_path)\n",
    "    elif pred_file_format == \"bio\":\n",
    "        pred = parse_bio(pred_path)\n",
    "    else:\n",
    "        pred = parse_json(pred_path)\n",
    " \n",
    "    \n",
    "    #print(gold)\n",
    "    #print(pred)\n",
    "        \n",
    "    #print(len(gold))\n",
    "    #print(len(pred))\n",
    "    \n",
    "    #print()\n",
    "    if per_dataset:\n",
    "        gold = [[item for sublist in gold for item in sublist]]\n",
    "        pred = [[item for sublist in pred for item in sublist]]\n",
    "    \n",
    "    # TODO 2. count the TP, TN, FP, FN\n",
    "    #         --> parameters: average (micro, macro), gold: <tid:[locs]>, pred: <tid:[locs]>\n",
    "    #         --> TP, TN, FP, FN\n",
    "    # TODO 3. compute P, R, and F_b --> parameters: \n",
    "    #         --> TP, TN, FP, FN\n",
    "    #         --> P, R, F_b\n",
    "    p = compute_precision(gold, pred, mode)\n",
    "    r = compute_recall(gold, pred, mode)\n",
    "    f = compute_fscore(beta, gold, pred, mode)\n",
    "\n",
    "    return p, r, f\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type-based evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_type(annot, typ):\n",
    "    annot_type = []\n",
    "    for i in range(len(annot)):\n",
    "        a = []\n",
    "        for j in range(len(annot[i])):\n",
    "            if annot[i][j][1] == typ:\n",
    "                a.append(annot[i][j])\n",
    "                #print(annot[i][j])\n",
    "                #print(a)\n",
    "                #print()\n",
    "        annot_type.append(a)\n",
    "\n",
    "        #print(annot_type)\n",
    "        #print()\n",
    "        #print()\n",
    "    return annot_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_type2(annot, typ):\n",
    "    annot_type = []\n",
    "    for i in range(len(annot)):\n",
    "        a = []\n",
    "        if annot[i][1] == typ:\n",
    "            a.append(annot[i])\n",
    "            #print(annot[i])\n",
    "            #print(a)\n",
    "            #print()\n",
    "        annot_type.append(a)\n",
    "\n",
    "        #print(annot_type)\n",
    "        #print()\n",
    "        #print()\n",
    "    return annot_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_by_type(gold_path, gold_file_format, pred_path, pred_file_format, beta, LOC, average, e_method):\n",
    "    #DRIVER code\n",
    "\n",
    "    # TODO 1. read the gold data \n",
    "    #         --> parameters: path, data format (BILOU, JSON)\n",
    "    #         --> output gold: [[locs]], pred: [[locs]]\n",
    "    gold = parse_bilou_by_type(gold_path) if gold_file_format == \"bilou\" else parse_json_by_type(gold_path, LOC)\n",
    "    pred = parse_bilou_by_type(pred_path) if pred_file_format == \"bilou\" else parse_json_by_type(pred_path, LOC)\n",
    "    \n",
    "    #print(gold)\n",
    "    #print(pred)\n",
    "    \n",
    "    f_gold = {item[1] for sublist in gold for item in sublist}\n",
    "    f_pred = {item[1] for sublist in pred for item in sublist}\n",
    "    types_to_eval = f_gold | f_pred\n",
    "    #print(types_to_eval)\n",
    "        \n",
    "    ps = [] #every entry for one type\n",
    "    rs = []\n",
    "    fs = []\n",
    "    \n",
    "    #gold = [[item for sublist in gold[:10] for item in sublist]]\n",
    "    #pred = [[item for sublist in pred[:10] for item in sublist]]\n",
    "    #print(gold)\n",
    "    #print(pred)\n",
    "    \n",
    "    if average == \"macro\":\n",
    "        for typ in types_to_eval:\n",
    "            gold_typ = filter_by_type(gold, typ)\n",
    "            pred_typ = filter_by_type(pred, typ)\n",
    "            #print(gold_typ)\n",
    "            #print(pred_typ)\n",
    "            ps.append(compute_precision(gold_typ, pred_typ, e_method))\n",
    "            rs.append(compute_recall(gold_typ, pred_typ, e_method))\n",
    "            fs.append(compute_fscore(beta, gold_typ, pred_typ, e_method))\n",
    "            #print(ps)\n",
    "            #print(rs)\n",
    "            #print(fs)\n",
    "            \n",
    "        \n",
    "        \n",
    "        p = sum(ps)/len(ps)\n",
    "        r = sum(rs)/len(rs)\n",
    "        f = sum(fs)/len(fs)\n",
    "        #print([(x, y) for x, y in zip(types_to_eval, fs)])\n",
    "\n",
    "    else: #default average is micro\n",
    "        #FIXME the special case is not handled here! this case is not implemented correctly!\n",
    "        #some code in the last cell in this notebook\n",
    "        p = 0\n",
    "        r = 0\n",
    "        f = 0\n",
    "\n",
    "    return p, r, f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"...\"\n",
    "e_method = 'special'# or 'standard'\n",
    "beta = 1\n",
    "per_dataset = False\n",
    "\n",
    "for typ in ['typeless', 'typebased']:\n",
    "    for event in en_events:\n",
    "        pp = path + \"baselines/random/\" + typ + \"/CRF-\" + event + \".txt\"\n",
    "        gp = path + \"/random/\" + typ + \"/\" + event + \"/test.txt\"            \n",
    "        pm, rm, fm = evaluate(gp, \"bilou\", pp, \"bilou\", 1, e_method, per_dataset)\n",
    "        print(\"%s\\t%s\\t%f\\t%f\\t%f\" % (event, case, pm, rm, fm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"...\"\n",
    "e_method = 'special'# or 'standard'\n",
    "LOC = False\n",
    "average = 'macro'\n",
    "beta = 1\n",
    "\n",
    "for typ in ['typeless', 'typebased']:\n",
    "    for event in en_events:\n",
    "        pp = path + \"baselines/random/\" + typ + \"/CRF-\" + event + \".txt\"\n",
    "        gp = path + \"/random/\" + typ + \"/\" + event + \"/test.txt\"            \n",
    "        pm, rm, fm = evaluate_by_type(gp, \"json\", pp, \"bilou\", beta, LOC, average, e_method)\n",
    "        print(\"%s\\t%s\\t%f\\t%f\\t%f\" % (event, case, pm, rm, fm))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
