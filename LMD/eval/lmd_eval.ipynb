{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8910016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import codecs\n",
    "import unicodecsv\n",
    "import logging\n",
    "from itertools import combinations \n",
    "from ast import literal_eval\n",
    "from collections import defaultdict, namedtuple\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8be7ec99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NOMINATIM_LIMIT = 2500\n",
    "\n",
    "en_events = [\"california_wildfires_2018\", \"canada_wildfires_2016\", \"cyclone_idai_2019\", \"ecuador_earthquake_2016\", \n",
    "          \"greece_wildfires_2018\", \"hurricane_dorian_2019\", \"hurricane_florence_2018\", \"hurricane_harvey_2017\", \n",
    "          \"hurricane_irma_2017\", \"hurricane_maria_2017\", \"hurricane_matthew_2016\", \"italy_earthquake_aug_2016\", \n",
    "          \"kaikoura_earthquake_2016\", \"kerala_floods_2018\", \"maryland_floods_2018\", \"midwestern_us_floods_2019\", \n",
    "          \"pakistan_earthquake_2019\", \"puebla_mexico_earthquake_2017\", \"srilanka_floods_2017\"]\n",
    " \n",
    "ar_events = [\"beirut_explosion_2020\", \"cairo_bombing_2019\", \"covid_2019\", \"dragon_storms_2020\",\n",
    "             \"hafr_albatin_floods_2019\", \"jordan_floods_2018\", \"kuwait_floods_2018\"] \n",
    "\n",
    "\n",
    "parts = [\"train\", \"dev\", \"test\"]\n",
    "\n",
    "types_dict = {\"Island\": \"ISL\", \"State\": \"STAT\", \"Continent\": \"CONT\", \"City/town\": \"CITY\", \"Country\": \"CTRY\",\n",
    "        \"County\": \"CNTY\", \"Neighborhood\": \"NBHD\", \"Road/street\": \"ST\", \"District\": \"DIST\", \"Other locations\": \"OTHR\", \n",
    "        \"Natural Point-of-Interest\": \"NPOI\", \"Human-made Point-of-Interest\": \"HPOI\"}\n",
    "types = list(types_dict.values())\n",
    "\n",
    "TGs = {\"country\": [\"island\", \"state\", \"city\", \"country\", \"county\", \"neighborhood\", \"street\", \"district\", \"other locations\", \"natural point-of-interest\", \"human-made point-of-interest\"],\n",
    "       \"state\": [\"island\", \"state\", \"city\", \"county\", \"neighborhood\", \"street\", \"district\", \"other locations\", \"natural point-of-interest\", \"human-made point-of-interest\"], \n",
    "       \"county\": [\"island\", \"city\", \"county\", \"neighborhood\", \"street\", \"district\", \"other locations\", \"natural point-of-interest\", \"human-made point-of-interest\"],\n",
    "       \"city\": [\"city\", \"neighborhood\", \"street\", \"other locations\", \"natural point-of-interest\", \"human-made point-of-interest\"],\n",
    "       \"district\": [\"neighborhood\", \"street\", \"district\", \"other locations\", \"natural point-of-interest\", \"human-made point-of-interest\"],\n",
    "       \"neighborhood\": [\"neighborhood\"], #, \"street\", \"natural point-of-interest\", \"human-made point-of-interest\"],\n",
    "       \"street\": [\"street\", \"natural point-of-interest\", \"human-made point-of-interest\"],\n",
    "       \"point-of-interest\": [\"natural point-of-interest\", \"human-made point-of-interest\"]}\n",
    "        # \"other\": [\"island\", \"continent\", \"other locations\"],\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "855d6e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "def geocode(coords):\n",
    "    locs = []\n",
    "    locator = Nominatim(user_agent=\"LMDGeocoder\")\n",
    "    url = \"https://nominatim.openstreetmap.org/reverse?format=jsonv2&extratags=1&addressdetails=1\"\n",
    "    for coord in coords:\n",
    "        coordinates = \"&lat={}&lon={}\".format(coord[0], coord[1]) #\"53.480837, -2.244914\"\n",
    "        location = requests.get(url + coordinates, timeout=30)\n",
    "        locs.append(json.loads(location.content)) \n",
    "    return locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d873b9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def resolve_to_location(coords_dict, online, cached_coords=None, multiprocessing=False):\n",
    "\n",
    "    y_resolved_dict = {}\n",
    "    for k in coords_dict:\n",
    "        for sys in coords_dict[k]:\n",
    "            coords = coords_dict[k][sys]\n",
    "            unresolved = set(coords)\n",
    "\n",
    "            if cached_coords:\n",
    "                unresolved.difference_update(set(cached_coords.keys()))\n",
    "\n",
    "            if unresolved and online:\n",
    "                unresolved = list(unresolved)\n",
    "                size = min(len(unresolved), NOMINATIM_LIMIT)\n",
    "\n",
    "                locations = list(geocode(unresolved))\n",
    "                cached_coords.update([(coord, loc) for coord, loc in zip(unresolved, locations) if 'address' in loc or 'address_components' in loc])\n",
    "                logging.debug('Number of resolved locations {}/{}'.format(len([l for l in locations if 'address' in l or 'address_components' in l]), size))\n",
    "                logging.debug('Number of returned locations {}'.format(len(locations)))\n",
    "            else:\n",
    "                logging.info('Offline mode')\n",
    "                logging.info('Number of cached locations is {}, unresolved is {}'.format(len(cached_coords), len(unresolved)))\n",
    "\n",
    "\n",
    "            y_resolved = []\n",
    "            for latitude, longitude in coords:\n",
    "                location = cached_coords.get((latitude, longitude), {})\n",
    "                country = state = county = city = district = neighborhood = street = point_of_interest = 'unresolved'\n",
    "\n",
    "                country = location['address'].get('country', 'unresolved')\n",
    "                state = location['address'].get('state', 'unresolved')\n",
    "                if state == \"unresolved\":\n",
    "                    state = location['address'].get('province', 'unresolved')\n",
    "\n",
    "                county = location['address'].get('county', 'unresolved')\n",
    "                city = location['address'].get('city', 'unresolved')\n",
    "                if city == \"unresolved\":\n",
    "                    city = location['address'].get('village', 'unresolved')\n",
    "\n",
    "                district = location['address'].get('district', 'unresolved')\n",
    "                if district == \"unresolved\": \n",
    "                    district = location['address'].get('subdistrict', 'unresolved')\n",
    "\n",
    "                neighborhood = location['address'].get('neighborhood', 'unresolved')\n",
    "                if neighborhood == \"unresolved\": #https://en.wikipedia.org/wiki/Suburb\n",
    "                    neighborhood = location['address'].get('subrub', 'unresolved')\n",
    "\n",
    "                street = location['address'].get('street', 'unresolved')\n",
    "                if street == \"unresolved\":\n",
    "                    street = location['address'].get('road', 'unresolved')\n",
    "\n",
    "                #Read more: https://wiki.openstreetmap.org/wiki/Key:addr:*\n",
    "                point_of_interest = location['address'].get('house_number', 'unresolved')\n",
    "                if point_of_interest == \"unresolved\":\n",
    "                    point_of_interest = location['address'].get('house_name', 'unresolved')\n",
    "                if point_of_interest == \"unresolved\":\n",
    "                    point_of_interest = location['address'].get('flats', 'unresolved')\n",
    "                if point_of_interest == \"unresolved\":\n",
    "                    point_of_interest = location['address'].get('place', 'unresolved')\n",
    "                if point_of_interest == \"unresolved\":\n",
    "                    point_of_interest = location['address'].get('hamlet', 'unresolved')\n",
    "                if point_of_interest == \"unresolved\":\n",
    "                    point_of_interest = location['address'].get('flats', 'unresolved')\n",
    "                if point_of_interest == \"unresolved\": #https://wiki.openstreetmap.org/wiki/Key:amenity\n",
    "                    point_of_interest = location['address'].get('amenity', 'unresolved')                \n",
    "\n",
    "                y_resolved.append({\"latitude\":latitude, \"longitude\":longitude, #\"type\": typ,\n",
    "                                   \"country\":country.lower(),\n",
    "                                   \"state\":state.lower(),\n",
    "                                   \"city\":city.lower(),\n",
    "                                   \"county\":county.lower()})\n",
    "                                   #\"district\":county.lower(),\n",
    "                                   #\"street\":county.lower(),\n",
    "                                   #\"island\":county.lower(),\n",
    "                                   #\"nbhd\":county.lower(),\n",
    "                                   #\"hpoi\":county.lower(),\n",
    "                                   #\"mpoi\":county.lower()})\n",
    "\n",
    "            if k not in y_resolved_dict:\n",
    "                y_resolved_dict[k] = {}\n",
    "            y_resolved_dict[k][sys] = y_resolved\n",
    "            \n",
    "    return y_resolved_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48ee69a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def geoloceval(docs):\n",
    "    systems = defaultdict(list)\n",
    "    for d, v in docs.items():\n",
    "        for s, r in v.items():\n",
    "            systems[s].append(r)\n",
    "            \n",
    "    target = systems.pop('gold')\n",
    "    \n",
    "    granularities = list(set(target[0][0].keys()).difference(set(['tweet_id', 'latitude', 'longitude', 'type'])))\n",
    "    print(\"granularities\")\n",
    "    print(granularities)\n",
    "    results = []\n",
    "    Result = namedtuple('Results', ['System', 'Gran', 'MRR1', 'MRR3', 'MRR5'])\n",
    "    \n",
    "    print(\"System\\tGran\\tMRR@1\\tMRR@3\\tMRR@5\")\n",
    "    # Evaluation per granularity\n",
    "    for g in granularities:\n",
    "        \n",
    "        logging.info('Discrete Evaluation at the level of {} **************************************************'.format(g.upper()))\n",
    "        print('{}'.format(g.upper()), end='\\t')\n",
    "        # extract target labels on the level of g granularity\n",
    "        l_target_all = [d[0][g] for d in target]\n",
    "        l_type_all = [d[0]['type'] for d in target]\n",
    "        tg = TGs[g]\n",
    "        l_target = [d for i, d in enumerate(l_target_all) if l_type_all[i] in tg]\n",
    "               \n",
    "        lt_unresolved = len([p for p in l_target if p == 'unresolved'])\n",
    "        \n",
    "        if len(l_target) > 0:\n",
    "            logging.info('Number of originally unresolved locations: {} ({:.3%})'.format(lt_unresolved, (float(lt_unresolved)/len(l_target))))\n",
    "            print('Number of originally unresolved locations: {} ({:.3%})'.format(lt_unresolved, (float(lt_unresolved)/len(l_target))))\n",
    "        else:\n",
    "            logging.info('No sufficient gold LMs for evaluation at this granularity')\n",
    "            print('No sufficient gold LMs for evaluation at this granularity')\n",
    "            continue\n",
    "            \n",
    "        data = defaultdict(list)\n",
    "        \n",
    "        for s in systems:\n",
    "            l_pred_all_ranked = []\n",
    "            for l in systems[s]:\n",
    "                l_pred_all_ranked.append([d[g] if len(d) > 0 else 'unresolved' for d in l])\n",
    "            \n",
    "            l_pred_ranked = []\n",
    "            for i in range(len(l_pred_all_ranked)):\n",
    "                l = l_pred_all_ranked[i]\n",
    "                if l_type_all[i] not in tg: \n",
    "                    continue\n",
    "                l_pred_ranked.append([d for i, d in enumerate(l)])\n",
    "            \n",
    "            mrr1 = calculate_mrr(l_target, l_pred_ranked, 1)\n",
    "            mrr3 = calculate_mrr(l_target, l_pred_ranked, 3)\n",
    "            mrr5 = calculate_mrr(l_target, l_pred_ranked, 5)\n",
    "            \n",
    "\n",
    "            l_pred_all = [d[0][g] if len(d) > 0 else 'unresolved' for d in systems[s]]\n",
    "            l_pred = [d for i, d in enumerate(l_pred_all) if l_type_all[i] in tg]\n",
    "            \n",
    "            for i, e in enumerate(l_pred):\n",
    "                if e is None:\n",
    "                    l_pred[i] = 'unresolved'\n",
    "            \n",
    "            data[s] = l_pred\n",
    "            \n",
    "\n",
    "            lp_unresolved = len([p for p in l_pred if p == 'unresolved'])\n",
    "            logging.info('Number of unresolved locations for {}: {} ({:.3%})'.format(s, lp_unresolved, float(lp_unresolved)/len(l_pred)))\n",
    "            \n",
    "                        \n",
    "            results.append(Result._make((s, g, mrr1, mrr3, mrr5,)))\n",
    "            \n",
    "            print(\"{}\\t{}\\t{:.4f}\\t{:.4f}\\t{:.4f}\".format(s, g, mrr1, mrr3, mrr5))\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1d8d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: match using the toponym id not toponym name\n",
    "def calculate_mrr(gold, ranked_toponyms, cutoff):\n",
    "    RR = []\n",
    "    for i in range(len(gold)):\n",
    "        lm = gold[i]\n",
    "        rr = 0.0\n",
    "        if len(ranked_toponyms) > 0:\n",
    "            limit = min(len(ranked_toponyms[i]), cutoff)\n",
    "            for j in range(limit):\n",
    "                if ranked_toponyms[i][j].lower() == gold[i].lower():\n",
    "                    rr += 1.0/(j+1)\n",
    "                    break #to avoid scoring multiple toponyms with the same name correctly!\n",
    "        RR.append(rr)\n",
    "    mrr = sum(RR)/len(RR)\n",
    "    return mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db4635fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run(gold, systems, outdir, geoloc, g_online, p_online):\n",
    "    docs = defaultdict(dict)\n",
    "    cached_coords = {}\n",
    "    \n",
    "    if geoloc:\n",
    "        with codecs.open(geoloc, 'r', 'utf-8') as fsystem:  \n",
    "            for line in fsystem:\n",
    "                s = line.split(\"\\t\")\n",
    "                \n",
    "                cached_coords.update([(literal_eval(s[0]), literal_eval(s[1]))]) \n",
    "\n",
    "    # load the ground-truth\n",
    "    with codecs.open(gold, 'r', 'utf-8') as fgold:\n",
    "        logging.info('{}'.format(os.path.basename(gold)))\n",
    "        \n",
    "        for line in fgold:\n",
    "            data = json.loads(line)\n",
    "            tmp = data.pop('tweet_id') + \"-\" + data.pop('lm')\n",
    "            for i in [\"1\", \"2\", \"3\", \"4\", \"5\"]:\n",
    "                if tmp + \"-\" + i not in docs:\n",
    "                    tmp = tmp + \"-\" + i\n",
    "                    break\n",
    "            i = tmp\n",
    "            docs[i]['gold'] = [data]\n",
    "        \n",
    "        # resolve gold coordinates to locations\n",
    "        coords = {d:{'gold':[(float(docs[d]['gold'][0]['latitude']), float(docs[d]['gold'][0]['longitude']))]} for d in docs}\n",
    "        \n",
    "        resolved_coords = {}\n",
    "        resolved_coords_temp = resolve_to_location(coords, g_online, cached_coords)\n",
    "        resolved_coords = {d:{'gold':[]} for d in docs}\n",
    "                \n",
    "        for r, d in zip(resolved_coords_temp.values(), docs.keys()):\n",
    "            r['gold'][0]['type'] = docs[d]['gold'][0]['type']\n",
    "            resolved_coords[d]['gold'].append(r['gold'][0])\n",
    "\n",
    "        for k, v in zip(docs.keys(), resolved_coords.values()):\n",
    "            docs[k] = json.loads(json.dumps(v).lower())\n",
    "        \n",
    "    for s in systems:\n",
    "        with codecs.open(s, 'r', 'utf-8') as fsystem:           \n",
    "            sysname = os.path.basename(s).split('.')[0]\n",
    "            for line in fsystem:\n",
    "                data = json.loads(line)\n",
    "                tmp = data.pop('tweet_id') + \"-\" + data.pop('lm')\n",
    "                for i in [\"1\", \"2\", \"3\", \"4\", \"5\"]:\n",
    "                    if tmp + \"-\" + i in docs:\n",
    "                        if sysname not in docs[tmp + \"-\" + i]:\n",
    "                            tmp = tmp + \"-\" + i\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "                    else:\n",
    "                        tmp = tmp + \"-\" + i\n",
    "                        break\n",
    "                i = tmp\n",
    "                docs[i][sysname] = json.loads(json.dumps(data[\"ranked_toponyms\"]).lower())\n",
    "           \n",
    "            if (p_online):\n",
    "                coords = {}\n",
    "                for k in docs.keys(): \n",
    "                    d = docs[k]\n",
    "                    coords[k] = {}\n",
    "                    try:\n",
    "                        coords[k][sysname] = []                       \n",
    "                        for res in d[sysname]:\n",
    "                            coords[k][sysname].append((float(res['latitude']), float(res['longitude'])))\n",
    "                    except:\n",
    "                        coords[k][sysname] = []\n",
    "                        for res in d[sysname]:\n",
    "                            coords[k][sysname].append((0.0, 0.0))                            \n",
    "                resolved_coords = resolve_to_location(coords, p_online, cached_coords)\n",
    "            else:\n",
    "                resolved_coords = {}\n",
    "                for k in docs.keys(): \n",
    "                    d = docs[k]\n",
    "                    resolved_coords[k] = {}\n",
    "                    resolved_coords[k][sysname] = []\n",
    "                    for res in d[sysname]:\n",
    "                        resolved_coords[k][sysname].append(\n",
    "                            {\"latitude\":float(res['latitude'])\n",
    "                             , \"longitude\":float(res['longitude'])\n",
    "                             , \"country\":res['country'], \"state\":res['state']\n",
    "                             , \"city\":res['city'], \"county\":res['county']})\n",
    "\n",
    "            for k in docs.keys():\n",
    "                true_coords = (docs[k]['gold'][0]['latitude'], docs[k]['gold'][0]['longitude'])\n",
    "                for si in range(len(resolved_coords[k][sysname])):\n",
    "                    pred_coords = (float(docs[k][sysname][si]['latitude']), float(docs[k][sysname][si]['longitude']))\n",
    "                    v = resolved_coords[k][sysname][si]\n",
    "                    docs[k][sysname][si] = json.loads(json.dumps(v).lower())\n",
    "    \n",
    "    if not geoloc and (g_online or p_online):\n",
    "        geoloc = os.path.join(outdir, '.coords_cache.txt')\n",
    "    \n",
    "    if geoloc:\n",
    "        with codecs.open(geoloc, 'w', 'utf-8') as outf:\n",
    "            for k, v in cached_coords.items():\n",
    "                outf.write('{}\\t{}\\n'.format(k, v))\n",
    "                    \n",
    "            logging.info('Updated the offline coordinates cache.')\n",
    "    \n",
    "    outfile = os.path.join(outdir, 'eval.output.json')\n",
    "    with codecs.open(outfile, 'w', 'utf-8') as jfile:\n",
    "        json.dump(docs, jfile, ensure_ascii=False, indent=4, separators=(',', ':'))\n",
    "    logging.info('Saved formatted files to {}'.format(outfile))\n",
    "\n",
    "    \n",
    "    geoloceval(docs)\n",
    "\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801534e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lap= \"msuwa\" #\"QCRI-IMMRAN\" #\"msuwa\"\n",
    "g = \"demo\\\\lat-lon-mapping\\\\\" #path to the mapping of lms to their ground truth geo-coordinates\n",
    "o = \"demo\\\\results\\\\\" #path where results will be logged\n",
    "p = \"demo\\\\baselines\\\\\" #path to the predictions of all systems\n",
    "c = \"demo\\\\nominatim-cache\\\\\" #'Filepath for cached geographic location mappings'. set to None when unavailable\n",
    "\n",
    "events = [\"sample\"]\n",
    "\n",
    "g_online = False #'Use an online geocoding API; default=False (offline)'\n",
    "p_online = True \n",
    "ss = [\"georeferencing\", \"geolocator2\", \"geolocator3\", \"geoparsepy\", \"nominatim\"]\n",
    "for e in events:\n",
    "    systems = []\n",
    "    for s in ss:\n",
    "        systems.append(p + \"\\\\\" + e + \"\\\\\" + s + \".jsonl\")\n",
    "    geoloc = c + e + \"\\\\test.txt\" #\n",
    "    gold = g + e + \"\\\\test.jsonl\"\n",
    "    outdir = o\n",
    "    run(gold, systems, outdir, geoloc, g_online, p_online)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fb30a7-5de6-4f3c-b4bb-df62926f6ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363c05c3-a32f-4c4e-b315-9d010cae0ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
